{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-10T06:27:09.576958Z",
     "iopub.status.busy": "2025-02-10T06:27:09.576669Z",
     "iopub.status.idle": "2025-02-10T06:27:14.681352Z",
     "shell.execute_reply": "2025-02-10T06:27:14.680298Z",
     "shell.execute_reply.started": "2025-02-10T06:27:09.576936Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:27:14.683114Z",
     "iopub.status.busy": "2025-02-10T06:27:14.682804Z",
     "iopub.status.idle": "2025-02-10T06:27:37.401015Z",
     "shell.execute_reply": "2025-02-10T06:27:37.400388Z",
     "shell.execute_reply.started": "2025-02-10T06:27:14.683082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from transformers import AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorWithPadding\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import math\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import torch\n",
    "\n",
    ">>> torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:27:37.403020Z",
     "iopub.status.busy": "2025-02-10T06:27:37.402477Z",
     "iopub.status.idle": "2025-02-10T06:27:37.406530Z",
     "shell.execute_reply": "2025-02-10T06:27:37.405645Z",
     "shell.execute_reply.started": "2025-02-10T06:27:37.402996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AUTH_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:27:37.408430Z",
     "iopub.status.busy": "2025-02-10T06:27:37.407866Z",
     "iopub.status.idle": "2025-02-10T06:27:39.540858Z",
     "shell.execute_reply": "2025-02-10T06:27:39.539771Z",
     "shell.execute_reply.started": "2025-02-10T06:27:37.408401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/roberta-base', use_auth_token=AUTH_TOKEN)\n",
    "print(tokenizer.decode(tokenizer.encode(\"sinh viên đại học bách khoa hà nội\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:27:39.542697Z",
     "iopub.status.busy": "2025-02-10T06:27:39.542352Z",
     "iopub.status.idle": "2025-02-10T06:27:40.036568Z",
     "shell.execute_reply": "2025-02-10T06:27:40.035906Z",
     "shell.execute_reply.started": "2025-02-10T06:27:39.542666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "from glob import glob \n",
    "import re \n",
    "from nltk import word_tokenize as lib_tokenizer \n",
    " \n",
    "dict_map = dict({}) \n",
    " \n",
    "def word_tokenize(text): \n",
    "    global dict_map \n",
    "    words = text.split() \n",
    "    words_norm = [] \n",
    "    for w in words: \n",
    "        if dict_map.get(w, None) is None: \n",
    "            dict_map[w] = ' '.join(lib_tokenizer(w)).replace('``', '\"').replace(\"''\", '\"') \n",
    "        words_norm.append(dict_map[w]) \n",
    "    return words_norm \n",
    " \n",
    "def strip_answer_string(text): \n",
    "    text = text.strip() \n",
    "    while text[-1] in '.,/><;:\\'\"[]{}+=-_)(*&^!~`': \n",
    "        if text[0] != '(' and text[-1] == ')' and '(' in text: \n",
    "            break \n",
    "        if text[-1] == '\"' and text[0] != '\"' and text.count('\"') > 1: \n",
    "            break \n",
    "        text = text[:-1].strip() \n",
    "    while text[0] in '.,/><;:\\'\"[]{}+=-_)(*&^!~`': \n",
    "        if text[0] == '\"' and text[-1] != '\"' and text.count('\"') > 1: \n",
    "            break \n",
    "        text = text[1:].strip() \n",
    "    text = text.strip() \n",
    "    return text \n",
    " \n",
    "def strip_context(text): \n",
    "    text = text.replace('\\n', ' ') \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    text = text.strip() \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:27:40.037532Z",
     "iopub.status.busy": "2025-02-10T06:27:40.037301Z",
     "iopub.status.idle": "2025-02-10T06:29:47.545987Z",
     "shell.execute_reply": "2025-02-10T06:29:47.545243Z",
     "shell.execute_reply.started": "2025-02-10T06:27:40.037511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_parquet(\"/kaggle/input/msmarco-v11/ms_marco.v1.1.parquet\")\n",
    "df1.text = df1.text.apply(lambda x: \" \".join(word_tokenize(strip_context(x))))\n",
    "df1.question = df1.question.apply(lambda x: \" \".join(word_tokenize(strip_context(x))))\n",
    "df = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:29:47.547011Z",
     "iopub.status.busy": "2025-02-10T06:29:47.546777Z",
     "iopub.status.idle": "2025-02-10T06:29:47.684646Z",
     "shell.execute_reply": "2025-02-10T06:29:47.683960Z",
     "shell.execute_reply.started": "2025-02-10T06:29:47.546990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'is_selected': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:29:47.686645Z",
     "iopub.status.busy": "2025-02-10T06:29:47.686431Z",
     "iopub.status.idle": "2025-02-10T06:29:47.693053Z",
     "shell.execute_reply": "2025-02-10T06:29:47.692424Z",
     "shell.execute_reply.started": "2025-02-10T06:29:47.686627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class PairwiseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(PairwiseModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name, use_auth_token=AUTH_TOKEN)\n",
    "        self.config = AutoConfig.from_pretrained(model_name, use_auth_token=AUTH_TOKEN)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, ids, masks):\n",
    "        out = self.model(input_ids=ids,\n",
    "                           attention_mask=masks,\n",
    "                           output_hidden_states=False).last_hidden_state\n",
    "        out = out[:,0]\n",
    "        outputs = self.fc(out)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:35:20.164984Z",
     "iopub.status.busy": "2025-02-10T06:35:20.164658Z",
     "iopub.status.idle": "2025-02-10T06:35:20.171523Z",
     "shell.execute_reply": "2025-02-10T06:35:20.170476Z",
     "shell.execute_reply.started": "2025-02-10T06:35:20.164962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df.dropna().reset_index(drop=True)\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.content1 = tokenizer.batch_encode_plus(list(df.question.apply(lambda x: x.replace(\"_\",\" \")).values), max_length=max_length, truncation=True)[\"input_ids\"]\n",
    "        self.content2 = tokenizer.batch_encode_plus(list(df.text.apply(lambda x: x.replace(\"_\",\" \")).values), max_length=max_length, truncation=True)[\"input_ids\"]\n",
    "        self.targets = self.df.label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index >= len(self.df):\n",
    "            print(f\"Invalid index {index}, Dataset length: {len(self.df)}\")\n",
    "        return {\n",
    "            'ids1': torch.tensor(self.content1[index], dtype=torch.long),\n",
    "            'ids2': torch.tensor(self.content2[index][1:], dtype=torch.long),\n",
    "            'target': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:35:20.173107Z",
     "iopub.status.busy": "2025-02-10T06:35:20.172868Z",
     "iopub.status.idle": "2025-02-10T06:35:20.192108Z",
     "shell.execute_reply": "2025-02-10T06:35:20.191395Z",
     "shell.execute_reply.started": "2025-02-10T06:35:20.173088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pad_token_id = tokenizer.pad_token_id\n",
    "def collate_fn(batch):\n",
    "    ids = [torch.cat([x[\"ids1\"], x[\"ids2\"]]) for x in batch]\n",
    "    targets = [x[\"target\"] for x in batch]\n",
    "    max_len = np.max([len(x) for x in ids])\n",
    "    masks = []\n",
    "    for i in range(len(ids)):\n",
    "        if len(ids[i]) < max_len:\n",
    "            ids[i]= torch.cat((ids[i], torch.tensor([pad_token_id,]*(max_len - len(ids[i])),dtype=torch.long)))\n",
    "        masks.append(ids[i] != pad_token_id)\n",
    "    # print(tokenizer.decode(ids[0]))\n",
    "    outputs = {\n",
    "        \"ids\": torch.vstack(ids),\n",
    "        \"masks\": torch.vstack(masks),\n",
    "        \"target\": torch.vstack(targets).view(-1)\n",
    "    }\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:35:20.193709Z",
     "iopub.status.busy": "2025-02-10T06:35:20.193484Z",
     "iopub.status.idle": "2025-02-10T06:35:20.209853Z",
     "shell.execute_reply": "2025-02-10T06:35:20.209188Z",
     "shell.execute_reply.started": "2025-02-10T06:35:20.193690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:35:20.210901Z",
     "iopub.status.busy": "2025-02-10T06:35:20.210662Z",
     "iopub.status.idle": "2025-02-10T06:35:20.225038Z",
     "shell.execute_reply": "2025-02-10T06:35:20.224444Z",
     "shell.execute_reply.started": "2025-02-10T06:35:20.210872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def optimizer_scheduler(model, num_train_steps):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    opt = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    sch = get_linear_schedule_with_warmup(\n",
    "        opt,\n",
    "        num_warmup_steps=int(0.05*num_train_steps),\n",
    "        num_training_steps=num_train_steps,\n",
    "        last_epoch=-1,\n",
    "    )\n",
    "    return opt, sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T06:35:20.225853Z",
     "iopub.status.busy": "2025-02-10T06:35:20.225668Z",
     "iopub.status.idle": "2025-02-10T06:35:20.240303Z",
     "shell.execute_reply": "2025-02-10T06:35:20.239528Z",
     "shell.execute_reply.started": "2025-02-10T06:35:20.225837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-10T07:20:01.575Z",
     "iopub.execute_input": "2025-02-10T06:35:20.315566Z",
     "iopub.status.busy": "2025-02-10T06:35:20.315247Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "epochs = 5\n",
    "accumulation_steps = 8\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "error_ids = None\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(df, df.label)):\n",
    "    if fold != 0:\n",
    "        break\n",
    "    model = PairwiseModel('FacebookAI/roberta-base')\n",
    "    # model.load_state_dict(torch.load(f\"./outputs/pairwise_v2.bin\"))\n",
    "    model.cuda()\n",
    "    train_df = df\n",
    "    # train_df = df.iloc[train_index].reset_index(drop=True)\n",
    "    val_df = df.iloc[test_index].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = SiameseDataset(train_df, tokenizer, 384)\n",
    "    valid_dataset = SiameseDataset(val_df, tokenizer, 384)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, collate_fn=collate_fn,\n",
    "                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=32, collate_fn=collate_fn,\n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    num_train_steps = len(train_loader) * epochs // accumulation_steps\n",
    "    optimizer, scheduler = optimizer_scheduler(model, num_train_steps)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "        for step, data in bar:\n",
    "            ids = data[\"ids\"].cuda()\n",
    "            # for x in ids:\n",
    "            #     print(tokenizer.decode(x))\n",
    "            masks = data[\"masks\"].cuda()\n",
    "            target = data[\"target\"].cuda()\n",
    "            # with torch.cuda.amp.autocast():\n",
    "            preds = model(ids, masks)\n",
    "            # print(preds.view(-1))\n",
    "            loss = loss_fn(preds.view(-1), target.view(-1))\n",
    "            loss /= accumulation_steps\n",
    "            loss.backward()\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                # scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "            bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            bar = tqdm(enumerate(valid_loader), total=len(valid_loader), leave=False)\n",
    "            targets = []\n",
    "            all_preds = []\n",
    "            for step, data in bar:\n",
    "                ids = data[\"ids\"].cuda()\n",
    "                masks = data[\"masks\"].cuda()\n",
    "                target = data[\"target\"].cuda()\n",
    "                preds = torch.sigmoid(model(ids, masks))\n",
    "                all_preds.extend(preds.cpu().view(-1).numpy())\n",
    "                targets.extend(target.cpu().view(-1).numpy())\n",
    "            all_preds = np.array(all_preds)\n",
    "            targets = np.array(targets)\n",
    "            print(f\"F1 {f1_score(targets, all_preds > 0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-10T07:20:01.575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"F1 {recall_score(np.array(targets), np.array(all_preds) > 0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-10T07:20:01.575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./kaggle/working/pairwise_msmarco.v1.1.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6636327,
     "sourceId": 10708021,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
